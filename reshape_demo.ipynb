{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1， reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 2, 2])\n",
      "torch.Size([60])\n",
      "tensor([ 0.2291, -0.3142,  0.5660,  0.1660, -0.6822, -0.8333, -0.4968, -0.4572,\n",
      "        -0.2140, -1.2602,  0.0688,  0.7145, -0.7998,  1.2341,  0.1150,  2.4299,\n",
      "         0.5717, -1.5586,  0.6175,  1.1083,  0.4488,  0.6110,  0.4972, -0.4272,\n",
      "        -1.1512, -0.8648, -0.9167,  0.9993, -0.8286,  0.7657,  1.0027,  0.7891,\n",
      "         0.3258, -0.0742,  0.5012, -1.4614,  0.9839,  1.7257,  0.3617, -1.3706,\n",
      "         0.5814, -1.0665,  0.5048, -1.3078,  0.5859, -0.7169, -1.5131,  0.2691,\n",
      "         0.4479,  2.2167, -0.8810, -0.9193,  0.7564, -0.0050, -4.0736, -1.2540,\n",
      "         1.4856,  1.0118,  0.9243,  0.2119])\n"
     ]
    }
   ],
   "source": [
    "batch = 5\n",
    "channel_num = 3\n",
    "width = 2\n",
    "height = 2\n",
    "tensor1 = torch.randn((batch, channel_num, width, height))\n",
    "print(tensor1.shape)\n",
    "#print(tensor1)\n",
    "tensor2 = tensor1.reshape(-1) #由原来的四个维度变成了一个维度，相当于把四维tensor，拉升成为了一个一维tensor\n",
    "print(tensor2.shape)\n",
    "print(tensor2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# numpy中的reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4)\n",
      "(16,)\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]\n",
      "(16, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "z = np.array([[1, 2, 3, 4],[5, 6, 7, 8],[9, 10, 11, 12],[13, 14, 15, 16]])\n",
    "print(z.shape)\n",
    "z_flatten = z.reshape(-1) #将z变成一个维度的numpy数组\n",
    "print(z_flatten.shape)\n",
    "print(z_flatten)\n",
    "z_col = z.reshape(-1, 1) #将z变成n行1列的2维数组\n",
    "print(z_col.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch 中retain_graph==True的作用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.csdn.net/qq_39861441/article/details/104129368\n",
    "\n",
    "总的来说进行一次backward之后，计算图会被清除，计算图中各节点的值依然是存在的，只是计算图被清除了。这样进行第二次backward会报错，如果加上retain_graph==True后,可以再来一次backward。\n",
    " \n",
    "retain_graph参数的作用\n",
    "官方定义：\n",
    "\n",
    "retain_graph (bool, optional) – If False, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Defaults to the value of create_graph.\n",
    "\n",
    "大意是如果设置为False，计算图中的中间变量在计算完后就会被释放。但是在平时的使用中这个参数默认都为False从而提高效率，和creat_graph的值一样。\n",
    "\n",
    "具体看一个例子理解：\n",
    "\n",
    "假设一个我们有一个输入x，y = x **2, z = y*4，然后我们有两个输出，一个output_1 = z.mean()，另一个output_2 = z.sum()。然后我们对两个output执行backward。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3943,  1.6039, -0.6963, -0.5955]], requires_grad=True)\n",
      "tensor([[0.1554, 2.5725, 0.4849, 0.3547]], grad_fn=<PowBackward0>)\n",
      "tensor([[ 0.6218, 10.2901,  1.9395,  1.4187]], grad_fn=<MulBackward0>)\n",
      "tensor(3.5675, grad_fn=<MeanBackward0>) tensor(14.2701, grad_fn=<SumBackward0>)\n",
      "tensor(3.5675, grad_fn=<MeanBackward0>) tensor(14.2701, grad_fn=<SumBackward0>)\n",
      "tensor([[-0.3943,  1.6039, -0.6963, -0.5955]], requires_grad=True)\n",
      "tensor([[0.1554, 2.5725, 0.4849, 0.3547]], grad_fn=<PowBackward0>)\n",
      "tensor([[ 0.6218, 10.2901,  1.9395,  1.4187]], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-4fc4164fcb52>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mloss2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;31m# 这时会引发错误\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.randn((1,4),dtype=torch.float32,requires_grad=True)\n",
    "y = x ** 2\n",
    "z = y * 4\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)\n",
    "loss1 = z.mean()\n",
    "loss2 = z.sum()\n",
    "print(loss1,loss2)\n",
    "loss1.backward()    # 这个代码执行正常，但是执行完中间变量都free了，所以下一个出现了问题\n",
    "print(loss1,loss2)\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)\n",
    "loss2.backward()    # 这时会引发错误"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "程序正常执行到第12行，所有的变量正常保存。但是在第13行报错：\n",
    "\n",
    "RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.\n",
    "\n",
    "分析：计算节点数值保存了，但是计算图x-y-z结构被释放了，而计算loss2的backward仍然试图利用x-y-z的结构，因此会报错。\n",
    "\n",
    "因此需要retain_graph参数为True去保留中间参数从而两个loss的backward()不会相互影响。正确的代码应当把第11行以及之后改成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4325, -0.0458,  0.9422, -0.6456]], requires_grad=True)\n",
      "tensor([[0.1871, 0.0021, 0.8878, 0.4168]], grad_fn=<PowBackward0>)\n",
      "tensor([[0.7483, 0.0084, 3.5510, 1.6673]], grad_fn=<MulBackward0>)\n",
      "tensor(1.4938, grad_fn=<MeanBackward0>) tensor(5.9750, grad_fn=<SumBackward0>)\n",
      "tensor(1.4938, grad_fn=<MeanBackward0>) tensor(5.9750, grad_fn=<SumBackward0>)\n",
      "tensor([[ 0.4325, -0.0458,  0.9422, -0.6456]], requires_grad=True)\n",
      "tensor([[0.1871, 0.0021, 0.8878, 0.4168]], grad_fn=<PowBackward0>)\n",
      "tensor([[0.7483, 0.0084, 3.5510, 1.6673]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.randn((1,4),dtype=torch.float32,requires_grad=True)\n",
    "y = x ** 2\n",
    "z = y * 4\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)\n",
    "loss1 = z.mean()\n",
    "loss2 = z.sum()\n",
    "print(loss1,loss2)\n",
    "loss1.backward(retain_graph=True)    # 这个参数表示保留计算图，\n",
    "print(loss1,loss2)\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)\n",
    "loss2.backward()    # 因此这里就不会发生错误了"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
